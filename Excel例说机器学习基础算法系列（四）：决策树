让我们从一个小的引例开始：
假设我们现在带着100块去超市买5样物品，临出发前将需要的物品列了清单：大米、耳机、吹风机、洗发水、蚊香。在到超市前我们不清楚每件物品的价格，因为预算有限，所以我们可能会有如下的思考过程：

购买的物品之所以有如下的顺序，可能的考量是大米保证基本的生活需求，蚊香保证基本的睡眠质量。比起耳机、洗发水、吹风机首先要买二者。在余额仍富余的情况下会考虑自己的头发质量，比起娱乐的耳机更重要，价格比起吹风机要实惠，所以先考量购买洗发水。通过以上考量①在所有的物品中最重要，②③④⑤则依次次之。
以上就是我们按照重要程度来购买物品的策略，决策树实际也是按照一系列的策略来最终达到我们的预期结果。只是考量因素的"重要程度"有所区别。
那么决策树划分因素的"重要程度"是什么呢？我们以西瓜书上的数据集作为引例，通过Excel详细进行推导后，再给定一些名词和总结，希望能够更好的理解决策树算法。

数据集：

步骤：
第一层节点划分：
1）数据集一共17个样本，其中好瓜的个数：8，坏瓜的个数：9。计算
            Ent（D）=  -\frac{8}{17}*log_2(\frac{8}{17})-\frac{9}{17}*log_2(\frac{9}{17}) = 0.998

2)依次按各属性对数据集进行划分，在给定某属性下有：
属性 = 色泽：
色泽=青绿的样本有6个，占整体样本比例 = \frac{6}{17} ，其中这6个样本中有好瓜3个，坏瓜3个。
色泽=乌黑的样本有6个，占整体样本比例 = \frac{6}{17} ，其中这6个样本中有好瓜4个，坏瓜2个。
色泽=浅白的样本有5个，占整体样本比例 = \frac{5}{17} ，其中这5个样本中有好瓜1个，坏瓜4个。

             Ent（色泽=青绿）=  -\frac{3}{6}*log_2(\frac{3}{6})-\frac{3}{6}*log_2(\frac{3}{6}) = 1

             Ent（色泽=乌黑）=    -\frac{4}{6}*log_2(\frac{4}{6})-\frac{2}{6}*log_2(\frac{2}{6}) = 0.918296

             Ent（色泽=浅白）=  -\frac{5}{6}*log_2(\frac{5}{6})-\frac{1}{6}*log_2(\frac{1}{6}) = 0.721928

         Gain（D,色泽）= 0.998 - ( \frac{6}{17} * 1 + \frac{6}{17} * 0.918296 + \frac{5}{17} *0.721928) = 0.10862
属性 = 根蒂：
根蒂=蜷缩的样本有8个，占整体样本比例 = \frac{8}{17} ，其中这8个样本中有好瓜5个，坏瓜3个。
根蒂=稍蜷的样本有7个，占整体样本比例 = \frac{7}{17} ，其中这7个样本中有好瓜3个，坏瓜4个。
根蒂=硬挺的样本有2个，占整体样本比例 = \frac{2}{17} ，其中这2个样本中有好瓜0个，坏瓜2个。

             Ent（根蒂=蜷缩）=  -\frac{5}{8}*log_2(\frac{5}{8})-\frac{3}{8}*log_2(\frac{3}{8}) = 0.954434  
             Ent（根蒂=稍蜷）=    -\frac{3}{7}*log_2(\frac{3}{7})-\frac{4}{7}*log_2(\frac{4}{7}) = 0.985228 
             Ent（根蒂=硬挺）=  -\frac{0}{2}*log_2(\frac{0}{2})-\frac{2}{2}*log_2(\frac{2}{2}) = 0 
            （规定： log_2(0) = 0 ）
     Gain（D,色泽）= 0.998 - ( \frac{8}{17} * 0.954434 + \frac{7}{17} * 0.985228  + \frac{2}{17} * 0) = 0.142675
其他属性计算方式与上述两个属性计算方式一致。



让我们先来梳理一下以上所做的计算：
① 计算数据集的信息熵：Ent（D）=  -\frac{8}{17}*log_2(\frac{8}{17})-\frac{9}{17}*log_2(\frac{9}{17}) = 0.998
信息熵：自信息的期望值，用来衡量信息量的大小。事情越确定（概率越大）其信息量越小，反之则信息量越大。按照公式① 衡量数据集的信息量大小（不确定性的大小）。
②  计算条件熵：H(D|a) = -\frac{6}{17} * 1 - \frac{6}{17} * 0.918296 - \frac{5}{17} *0.721928
条件熵：在给定某一特征，按该特征对数据集划分后信息量的大小。
③计算信息增益： Gain(D,a) = Ent(D) - H(D|a) 
信息增益：度量给定特征带来的不确定性降低的幅度。幅度越大说明用该属性用来划分数据集后的不确定性越小。
我们采用"幅度"较大的特征作为添加子树的依据，根据以上计算特征为纹理带来的信息增益最大，选用其先对数据集进行划分。于是有：

第二层节点划分：
按照属性中的枚举值划分的节点，节点中样本的类别一致不再进行划分，若类别不一致则需要继续进行划分。不再进行划分的节点也称为叶子节点。每次划分一次，用来划分的属性个数则减少一个（类别变量）。
我们选取按照纹理=清晰所划分的节点中的样本继续进行数据划分：

1）数据集一共9个样本，其中好瓜的个数：7，坏瓜的个数：2。计算信息熵：
            Ent（D|纹理=清晰）=  -\frac{7}{9}*log_2(\frac{7}{9})-\frac{2}{9}*log_2(\frac{2}{9}) = 0.764
2)依次按各属性对数据集进行划分，在给定某属性下有：
属性 = 色泽：
色泽=青绿的样本有4个，占整体样本比例 = \frac{4}{9} ，其中这4个样本中有好瓜3个，坏瓜1个。
色泽=乌黑的样本有4个，占整体样本比例 = \frac{4}{9} ，其中这4个样本中有好瓜3个，坏瓜1个。
色泽=浅白的样本有1个，占整体样本比例 = \frac{1}{9} ，其中这4个样本中有好瓜1个，坏瓜0个。

             Ent（色泽=青绿）=  -\frac{3}{4}*log_2(\frac{3}{4})-\frac{1}{4}*log_2(\frac{1}{4}) = 0.811
             Ent（色泽=乌黑）=  -\frac{3}{4}*log_2(\frac{3}{4})-\frac{1}{4}*log_2(\frac{1}{4}) = 0.811
             Ent（色泽=浅白）=  -\frac{1}{1}*log_2(\frac{1}{1})-\frac{0}{1}*log_2(\frac{0}{1}) = 0
         Gain（D,色泽）= 0.998 - ( \frac{4}{9} * 0.811 + \frac{4}{9} * 0.811+ \frac{1}{9} * 0 ) = 0.043
同理可计算其他属性的信息增益：


以上三个属性的信息增益均相等，可任选一个作为第二层节点。由此也知决策树不唯一。
为保持和西瓜书一致：我们不妨也取根蒂作为划分属性，于是有下图：

由上图我们可知还需要对纹理=清晰 \Rightarrow 根蒂=稍蜷路径进行再次划分。步骤与以上步骤相同。最终生成的树：

整颗树计算过程可具体查看附件：

决策树_西瓜书数据集.xlsx

